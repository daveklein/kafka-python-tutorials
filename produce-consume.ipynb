{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64dfead1",
   "metadata": {},
   "source": [
    "# Python Client Tutorial\n",
    "\n",
    "This tutorial introduces the Producer, Consumer, and AdminClient classes from the confluent-kafka package.\n",
    "\n",
    "Requirements:\n",
    "- Python 3\n",
    "- docker-compose (for running Kafka locally)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd269f",
   "metadata": {},
   "source": [
    "## Docker Compose\n",
    "\n",
    "The following cell will create the `docker-compose.yml` file that will be used to start an Apache Kafka broker, and a Zookeeper node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0787557-40b8-4836-a773-156ec811820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"---\n",
    "version: '2'\n",
    "\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:7.1.0\n",
    "    hostname: zookeeper\n",
    "    container_name: zookeeper\n",
    "    ports:\n",
    "      - \\\"2181:2181\\\"\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "\n",
    "  broker:\n",
    "    image: confluentinc/cp-kafka:7.1.0\n",
    "    hostname: broker\n",
    "    container_name: broker\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    ports:\n",
    "      - \\\"29092:29092\\\"\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n",
    "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\" > docker-compose.yml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5bc20e",
   "metadata": {},
   "source": [
    "Now we can run the following command to spin up our infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2fd54be-39b3-4ccd-966a-37aeb8c6b498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating network \"kafka-python-tutorials_default\" with the default driver\n",
      "Creating zookeeper ... \n",
      "Creating zookeeper ... done\n",
      "Creating broker    ... \n",
      "Creating broker    ... done\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8bbef7",
   "metadata": {},
   "source": [
    "## Confluent Kafka\n",
    "\n",
    "The classes that we will be working with in this tutorial, Producer, Consumer, and AdminClient, all reside in the `confluent-kafka` package, which we will install in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00e9943c-27ab-4456-9be1-fc76c4e4a7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: confluent-kafka in /Users/dklein/opt/anaconda3/lib/python3.9/site-packages (1.8.2)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install confluent-kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f567119d",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The `AdminClient`, `Producer`, and `Consumer` will all take a configuration dictionary in their constructor. For the first two, the only value we *have to* have is `bootstrap.servers`. So, we will start off by creating a dictionary with a `bootstrap.servers` that points to the Kafka broker we started with docker-compose.compile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e900ac9d-c877-4b54-b716-509472fba7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "          \"bootstrap.servers\":\"127.0.0.1:29092\"\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fa3581",
   "metadata": {},
   "source": [
    "## AdminClient\n",
    "\n",
    "Next let's use the `AdminClient` to create a new topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9acb830f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic hello_topic created\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "\n",
    "admin = AdminClient(config)\n",
    "result_dict = admin.create_topics([NewTopic(\"hello_topic\", num_partitions=1, replication_factor=1)])\n",
    "for topic, future in result_dict.items():\n",
    "    try:\n",
    "        future.result()  # The result itself is None\n",
    "        print(f\"Topic {topic} created\")\n",
    "    except Exception as e:\n",
    "            print(f\"Failed to create topic {topic}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0084df6",
   "metadata": {},
   "source": [
    "The previous cell should have shown a message confirming that our new topic was created, but we can also use the `list_topics()` method of the `AdminClient` to confirm that our new topic exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61256149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['hello_topic'])\n"
     ]
    }
   ],
   "source": [
    "md = admin.list_topics()\n",
    "print(md.topics.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abce2e9",
   "metadata": {},
   "source": [
    "## Producer\n",
    "\n",
    "Now we will use the `Producer` class to produce events to Kafka. First we need to import the `Producer` from the `confluent_kafka` package, then we can use the same config dictionay we used for our `AdminClient` to create our `Producer` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b858882-f868-44fe-9507-58e1f933aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "\n",
    "producer = Producer(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec7133",
   "metadata": {},
   "source": [
    "The `produce()` method of the `Producer` class is very straight forward and easy to use. So, let's use it to send a handful of `Hello world` events in different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6567467-80c5-46b9-98fd-feddc83b25ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.produce(\"hello_topic\", key=\"a\", value=\"Hello world!\")\n",
    "producer.produce(\"hello_topic\", key=\"a\", value=\"¡Hola Mundo!\")\n",
    "producer.produce(\"hello_topic\", key=\"a\", value=\"Hallo Wereld!\")\n",
    "producer.produce(\"hello_topic\", key=\"a\", value=\"Bonjour monde!\")\n",
    "producer.produce(\"hello_topic\", key=\"a\", value=\"Hej världen!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161086e1",
   "metadata": {},
   "source": [
    "## Consumer\n",
    "\n",
    "Our Hello world events have been produced, but it would be great to verify that. For this we'll use the `Consumer` class.\n",
    "\n",
    "In this next cell, we'll import the `Consumer` class and then we'll add a couple of consumer specific properties to our config dictionary. The default value for `auto.offset.reset` is `latest`, but if we leave that, our `Consumer` will not pick up the events we just produced. So, we'll set it to `earliest`. The other property we're adding is `group.id` and it can be set to any string value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2144bf41-94c2-47d1-8c4f-83b7db62ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer\n",
    "config[\"auto.offset.reset\"] = \"earliest\"\n",
    "config[\"group.id\"] = \"kafka-tutorial\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cf084c",
   "metadata": {},
   "source": [
    "Next we'll use the updated config dictionary to construct our `Consumer` instance. Before we can begin consuming events from a Kafka topic, we need to subscribe to that topic. The `subscribe()` method takes a list of topic names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ffcb1b3-ea0b-43c4-8c8c-eb5388193e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = Consumer(config)\n",
    "consumer.subscribe([\"hello_topic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac129d58",
   "metadata": {},
   "source": [
    "Now we can use the `poll()` method of the `Consumer` class to retrieve events from our Kafka topic. Often the `poll()` method will be used in a continuous loop, but for our purposes here, we will use a finite loop based on the number of events we produced earlier. Feel free to adjust this number if you had added more events in your favorite languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e583ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello world!'\n",
      "b'\\xc2\\xa1Hola Mundo!'\n",
      "b'Hallo Wereld!'\n",
      "b'Bonjour monde!'\n",
      "b'Hej v\\xc3\\xa4rlden!'\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    evt = consumer.poll(1.0)\n",
    "    if evt is None:\n",
    "        pass\n",
    "    else:\n",
    "        print(evt.value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e474a51",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Congratulations! You've completed this tutorial and have used the `AdminClient`, `Producer`, and `Consumer` classes of the `confluent-kafka` package. Now we will make one more call to `docker-compoase` to shut down the infrastructure we started up at the beginning of this tutoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e0dc1-9762-41c8-9e0c-1b93ece89ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker-compose down"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
